#! /usr/bin/python3
import sys
sys.path.extend( [ "../../lib", "../lib", "./lib" ] )

import traceback

import argparse
import numpy as np

from math import sqrt
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from keras.utils import np_utils
import keras

import NeuralNetwork as nn
import ModelAnalyzer as ma

import DataIO as dio
import Terminal as term
import Visual as visual

import types
import tempfile
import keras.models

def make_keras_picklable():
	def __getstate__(self):
		model_str = ""
		with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:
			keras.models.save_model(self, fd.name, overwrite=True)
			model_str = fd.read()
		d = { 'model_str': model_str }
		return d

	def __setstate__(self, state):
		with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:
			fd.write(state['model_str'])
			fd.flush()
			model = keras.models.load_model(fd.name)
		self.__dict__ = model.__dict__

	cls = keras.models.Model
	cls.__getstate__ = __getstate__
	cls.__setstate__ = __setstate__


preferredEngine = None
def set_preferred_engine( engine ):
	global preferredEngine
	if engine == "keras":
		preferredEngine = KerasSolver
		make_keras_picklable()
	else:
		preferredEngine = nn.NeuralNetworkSolver
	return engine

class KerasSolver:
	def __prepare_model( shaper, **kwArgs ):
		sampleSize = int( sqrt( shaper.feature_count() ) )
		topology = kwArgs.get( "nnTopology", [] )
		topology = [shaper.feature_count()] + topology + [shaper.class_count()]
		model = Sequential()
		model.add(Convolution2D(32, (3, 3), activation='relu', input_shape = ( sampleSize, sampleSize, 1 ) ) )
		model.add(Dropout(0.25))
		model.add(Convolution2D(32, (3, 3), activation='relu'))
		model.add(Convolution2D(32, (3, 3), activation='relu'))
		model.add(Dropout(0.25))
		model.add(MaxPooling2D(pool_size=(2,2)))
		model.add(Flatten())
		model.add(Dense(128, activation='relu'))
		model.add(Dropout(0.5))
		model.add(Dense(shaper.class_count(), activation='softmax'))
		model.compile( loss = 'categorical_crossentropy',
			optimizer = 'adam',
			metrics = ['accuracy']
		)

		return model

	def train( self_, X, y, **kwArgs ):
		shaper = ma.DataShaper( X, y, **kwArgs )
		sampleSize = int( sqrt( shaper.feature_count() ) )
		iters = kwArgs.get( "iters", 50 )
		Lambda = kwArgs.get( "Lambda", 0 )
		y = shaper.map_labels( y )
		y = keras.utils.to_categorical( y, num_classes = None )
		model = kwArgs.get( "model", KerasSolver.__prepare_model( shaper, **kwArgs ) )
		X = shaper.conform( X, addOnes = False )
		X = X.reshape( X.shape[0], sampleSize, sampleSize, 1 )
		X = X.astype('float32')
		model.fit( X, y, batch_size = 64, epochs = 20, verbose=1)
		return ma.Solution( model = model, shaper = shaper )

	def verify( self_, solution, X, y ):
		X = solution.shaper().conform( X, addOnes = False )
		sampleSize = int( sqrt( solution.shaper().feature_count() ) )
		X = X.reshape( X.shape[0], sampleSize, sampleSize, 1 )
		X = X.astype('float32')
		model = solution.model()
		yp = model.predict( X, verbose = 0 )
		yp = np.argmax( yp, axis = 1 )
		accuracy = np.mean( 1.0 * ( y.flatten() == solution.shaper().labels( yp ) ) )
		return 1 - accuracy

	def predict( self_, solution, X ):
		X = solution.shaper().conform( X, addOnes = False )
		sampleSize = int( sqrt( solution.shaper().feature_count() ) )
		X = X.reshape( X.shape[0], sampleSize, sampleSize, 1 )
		X = X.astype('float32')
		model = solution.model()
		yp = model.predict( X, verbose = 0 )
		yp = np.argmax( yp, axis = 1 )
		return solution.shaper().labels( yp )

def predict( solver, solution, X, y, n ):
	x = X[n]
	y = y[n]
	yp = solver.predict( solution, np.array( [ x ] ) )[0]
	if yp != y:
		term.plot( x, art = True, label = "n:{} p:{} e:{}".format( n, yp, y ) )
		return input( "Press `Enter` to continue or `q` followed by `Enter` to quit: " ) != "q"
	return True

def create( args ):
	labels = list( args.classes.split( "," ) )
	lifeDemo = visual.DrawingPad( labels = labels, repeat = args.samples, size = args.size )
	lifeDemo.run()
	dio.save( args.data_set, lifeDemo._data )

def train( args ):
	solver = preferredEngine()
	topology = list( args.topology.split( "," ) )
	Lambda = list( map( float, args.Lambda.split( "," ) ) )
	rawData = dio.load( args.data_set )
	X_orig = rawData["X"]
	y_orig = rawData["y"]
	optimizationResults = ma.find_solution(
		solver, X_orig, y_orig, showFailureRateTrain = True, iters = args.iterations,
		optimizationParams = {
			"nnTopology": [topology],
			"Lambda": Lambda,
			"functions": [
			]
		}
	)
	solution = optimizationResults.solution
	dio.save( args.solution, solution )
	if args.debug:
		print( "solution = {}".format( solution ) )
	for i in range( len( y_orig ) ):
		if not predict( solver, solution, X_orig, y_orig, i ):
			return

def test( args ):
	solver = preferredEngine()
	solution = dio.load( args.solution )
	rawData = dio.load( args.data_set )
	X_orig = rawData["X"]
	y_orig = rawData["y"]
	for i in range( len( y_orig ) ):
		if not predict( solver, solution, X_orig, y_orig, i ):
			return

def show( args ):
	solver = preferredEngine()
	solution = None
	yp = None

	rawData = dio.load( args.data_set )
	X_orig = rawData["X"]
	y_orig = rawData["y"]

	if args.solution:
		solution = dio.load( args.solution )
		yp = solver.predict( solution, X_orig )

	se = visual.SampleEditor( X_orig, y_orig, zoom = args.zoom, yPredict = yp )
	se.run()

def live( args ):
	solver = preferredEngine()
	solution = dio.load( args.solution )
	lifeDemo = visual.DrawingPad( solver = solver, solution = solution )
	lifeDemo.run()

def main():
	parser = argparse.ArgumentParser(
		description = "NeuralNetwork experimentation tool\n\n"
		"Example invocations:\n"
		"  {0} create -c classA,classB,classC -n 30 -d train_data_ABC.p\n"
		"  {0} train -d train_data.p -s solution_ABC.p -i 100 -t 40,25 -l 1,3,10\n"
		"  {0} test -d train_data.p -s solution_ABC.p\n"
		"  {0} show -d train_data.p -z 3 -s solution_ABC.p\n"
		"  {0} live -s solution_ABC.p".format( sys.argv[0] ),
		formatter_class = argparse.RawTextHelpFormatter
	)
	subparsers = parser.add_subparsers()

	parserCreate = subparsers.add_parser( "create", help = "Prepare new batch of training samples." )
	parserCreate.add_argument( "-d", "--data-set", metavar = "path", type = str, required = True, help = "Output file for created data set." )
	parserCreate.add_argument( "-c", "--classes", metavar = "labels", type = str, required = True, help = "List of classes to be added to new data set." )
	parserCreate.add_argument( "-n", "--samples", metavar = "num", type = int, required = True, help = "Number of samples per class." )
	parserCreate.add_argument( "-r", "--size", metavar = "size", type = int, required = True, help = "Single sample resolution." )
	parserCreate.set_defaults( func = create )

	parserTrain = subparsers.add_parser( "train", help = "Train given model on given data." )
	parserTrain.add_argument( "-d", "--data-set", metavar = "path", type = str, required = True, help = "Dataset for training." )
	parserTrain.add_argument( "-s", "--solution", metavar = "path", type = str, required = True, help = "Store solution path." )
	parserTrain.add_argument( "-t", "--topology", metavar = "topo", type = str, required = True, help = "NeuralNetwork topologies to test." )
	parserTrain.add_argument( "-l", "--Lambda", metavar = "lambda", type = str, required = True, help = "Values of regularization parameter to test." )
	parserTrain.add_argument( "-i", "--iterations", metavar = "num", type = int, help = "Maximum number of iterations." )
	parserTrain.set_defaults( iterations = 50 )
	parserTrain.set_defaults( func = train )

	parserTest = subparsers.add_parser( "test", help = "Test trained model against given data." )
	parserTest.add_argument( "-d", "--data-set", metavar = "path", type = str, required = True, help = "Dataset to test a model against." )
	parserTest.add_argument( "-s", "--solution", metavar = "path", type = str, required = True, help = "Path to solution to test." )
	parserTest.set_defaults( func = test )

	parserShow = subparsers.add_parser( "show", help = "Show sample data, optionally with invalid predictions from model." )
	parserShow.add_argument( "-d", "--data-set", metavar = "path", type = str, required = True, help = "Dataset for training." )
	parserShow.add_argument( "-s", "--solution", metavar = "path", type = str, help = "Load solution path." )
	parserShow.add_argument( "-z", "--zoom", metavar = "level", type = int, help = "Zoom level." )
	parserShow.set_defaults( zoom = 1 )
	parserShow.set_defaults( func = show )

	parserLive = subparsers.add_parser( "live", help = "Run live test for given solution." )
	parserLive.add_argument( "-s", "--solution", metavar = "path", type = str, required = True, help = "Solution to use for live test." )
	parserLive.set_defaults( func = live )

	parser.add_argument(
		"-e", "--engine", metavar = "name", choices = [ "struggle", "keras" ], type = set_preferred_engine,
		action = "store",
		help = "Choose machine learning engine."
	)
	parser.add_argument( "-v", "--verbose", help = "Increase program verbosity level.", action = 'store_true' )
	parser.add_argument( "-D", "--debug", help = "Print all debuging information.", action = 'store_true' )
	parser.set_defaults( engine = "keras" )
	parser.set_defaults( verbose = False )
	parser.set_defaults( debug = False )
	parser.set_defaults( func = lambda x : parser.print_help() )
	args = parser.parse_args()
	args.func( args )
	return

if __name__ == "__main__":
	try:
		main()
	except Exception:
		traceback.print_exc( file = sys.stdout )
		sys.exit( 1 )

